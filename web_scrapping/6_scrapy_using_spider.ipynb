{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scrapy import Spider, Request\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items). In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).\n",
    "\n",
    "##### For spiders, the scraping cycle goes through something like this:\n",
    "\n",
    "###### 1- You start by generating the initial Requests to crawl the first URLs, and specify a callback function to be called with the response downloaded from those requests.\n",
    "\n",
    "###### 2- The first requests to perform are obtained by calling the start_requests() method which (by default) generates Request for the URLs specified in the start_urls and the parse method as callback function for the Requests.\n",
    "\n",
    "###### 3- In the callback function, you parse the response (web page) and return item objects, Request objects, or an iterable of these objects. Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback.\n",
    "\n",
    "###### 4- In callback functions, you parse the page contents, typically using Selectors (but you can also use BeautifulSoup, lxml or whatever mechanism you prefer) and generate items with the parsed data.\n",
    "\n",
    "###### 5- Finally, the items returned from the spider will be typically persisted to a database (in some Item Pipeline) or written to a file using Feed exports.\n",
    "\n",
    "Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### name\n",
    "##### A string which defines the name for this spider. The spider name is how the spider is located (and instantiated) by Scrapy, so it must be unique. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and itâ€™s required.\n",
    "\n",
    "##### If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the TLD. So, for example, a spider that crawls mywebsite.com would often be called mywebsite."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class WikipediaSpider(Spider):\n",
    "    name = 'wiki_spider'\n",
    "    def start_requests(self):\n",
    "        urls = ['https://en.wikipedia.org/wiki/Web_scraping']\n",
    "        for url in urls:\n",
    "            yield Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        html_file = 'DC_Courses.html'\n",
    "        with open(html_file, 'wb') as fout:\n",
    "            fout.write(response.body)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:02:58 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-02-16 19:02:58 [scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.10.0 (default, Dec 21 2021, 13:36:04) [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.0, Platform Linux-5.4.0-96-generic-x86_64-with-glibc2.27\n",
      "2022-02-16 19:02:58 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:03:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2022-02-16 19:03:01 [scrapy.extensions.telnet] INFO: Telnet Password: 15993518ff3dcd14\n",
      "2022-02-16 19:03:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-02-16 19:03:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-02-16 19:03:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-02-16 19:03:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-02-16 19:03:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-02-16 19:03:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-02-16 19:03:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Deferred at 0x7f1bd696fc10>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.crawl(WikipediaSpider)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:03:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Web_scraping> (referer: None)\n",
      "2022-02-16 19:03:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-02-16 19:03:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 237,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29040,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 3.699937,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 2, 16, 17, 3, 5, 401315),\n",
      " 'httpcompression/response_bytes': 110438,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 88592384,\n",
      " 'memusage/startup': 88592384,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 2, 16, 17, 3, 1, 701378)}\n",
      "2022-02-16 19:03:05 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 22:25:05 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
      "2022-02-15 22:25:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 11.318816,\n",
      " 'finish_reason': 'shutdown',\n",
      " 'finish_time': datetime.datetime(2022, 2, 15, 20, 25, 5, 65914),\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 136728576,\n",
      " 'memusage/startup': 136728576,\n",
      " 'start_time': datetime.datetime(2022, 2, 15, 20, 24, 53, 747098)}\n",
      "2022-02-15 22:25:05 [scrapy.core.engine] INFO: Spider closed (shutdown)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<DeferredList at 0x7f851ebfead0 current result: [(True, None)]>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}